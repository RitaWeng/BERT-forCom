{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:12.880467Z",
     "start_time": "2019-12-30T09:45:10.018888Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from IPython.display import clear_output\n",
    "from transformers import *\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn import metrics\n",
    "import os\n",
    "#PRETRAINED_MODEL_NAME = \"bert-base-chinese\"  # 指定繁簡中文 BERT-BASE 預訓練模型\n",
    "PRETRAINED_MODEL_NAME = \"bert-large-cased\"\n",
    "#通常英文的case使用的模型\n",
    "# 取得此預訓練模型所使用的 tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "clear_output()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:12.884723Z",
     "start_time": "2019-12-30T09:45:12.882460Z"
    }
   },
   "outputs": [],
   "source": [
    "# all_df = pd.read_pickle('training_set.pkl')\n",
    "# validation_df = all_df.sample(4686)\n",
    "# all_df = all_df.drop(validation_df.index)\n",
    "# #切dataframe的時候 要記得把index reset\n",
    "# train_df = all_df.reset_index(drop=True)\n",
    "# validation_df = validation_df.reset_index(drop=True)\n",
    "# test_df = pd.read_pickle('public_test_cut.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:12.946663Z",
     "start_time": "2019-12-30T09:45:12.897396Z"
    }
   },
   "outputs": [],
   "source": [
    "all_df = pd.read_pickle(\"training_set_withOrder.pkl\")\n",
    "all_df = all_df.sample(frac=1) #shuffle training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:13.638125Z",
     "start_time": "2019-12-30T09:45:13.526167Z"
    }
   },
   "outputs": [],
   "source": [
    "#validation_df = all_df.sample(4682)\n",
    "\n",
    "train_pct_index = int(0.9 * len(all_df))\n",
    "train_df, validation_df = all_df[:train_pct_index], all_df[train_pct_index:]\n",
    "#y_train, y_test = y[:train_pct_index], y[train_pct_index:]\n",
    "\n",
    "#切dataframe的時候 要記得把index reset\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "validation_df = validation_df.reset_index(drop=True)\n",
    "test_df = pd.read_pickle('all_test_cut_withOrder.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:19.423105Z",
     "start_time": "2019-12-30T09:45:19.400075Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>Authors</th>\n",
       "      <th>Categories</th>\n",
       "      <th>Created Date</th>\n",
       "      <th>noTask1</th>\n",
       "      <th>Position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T00001_S001</td>\n",
       "      <td>Cheating-Resilient Incentive Scheme for Mobile...</td>\n",
       "      <td>Mobile Crowdsensing is a promising paradigm fo...</td>\n",
       "      <td>Zhao/Yang/Yu/Yao/Lin/Li</td>\n",
       "      <td>cs.NI/cs.CR</td>\n",
       "      <td>2017-01-08</td>\n",
       "      <td></td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T00001_S002</td>\n",
       "      <td>Cheating-Resilient Incentive Scheme for Mobile...</td>\n",
       "      <td>As a fundamental property of Mobile Crowdsensi...</td>\n",
       "      <td>Zhao/Yang/Yu/Yao/Lin/Li</td>\n",
       "      <td>cs.NI/cs.CR</td>\n",
       "      <td>2017-01-08</td>\n",
       "      <td></td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T00001_S003</td>\n",
       "      <td>Cheating-Resilient Incentive Scheme for Mobile...</td>\n",
       "      <td>Therefore, a mechanism is required for the sys...</td>\n",
       "      <td>Zhao/Yang/Yu/Yao/Lin/Li</td>\n",
       "      <td>cs.NI/cs.CR</td>\n",
       "      <td>2017-01-08</td>\n",
       "      <td></td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T00001_S004</td>\n",
       "      <td>Cheating-Resilient Incentive Scheme for Mobile...</td>\n",
       "      <td>In this paper, we develop a novel Cheating-Res...</td>\n",
       "      <td>Zhao/Yang/Yu/Yao/Lin/Li</td>\n",
       "      <td>cs.NI/cs.CR</td>\n",
       "      <td>2017-01-08</td>\n",
       "      <td></td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T00001_S005</td>\n",
       "      <td>Cheating-Resilient Incentive Scheme for Mobile...</td>\n",
       "      <td>Via theoretical analysis, we demonstrate the c...</td>\n",
       "      <td>Zhao/Yang/Yu/Yao/Lin/Li</td>\n",
       "      <td>cs.NI/cs.CR</td>\n",
       "      <td>2017-01-08</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262943</th>\n",
       "      <td>T40000_S005</td>\n",
       "      <td>A Mobile Phone based Speech Therapist</td>\n",
       "      <td>Speech therapy is critical for continuous impr...</td>\n",
       "      <td>Pandey/Pande/Kopparapu</td>\n",
       "      <td>cs.CY/cs.HC</td>\n",
       "      <td>2016-01-11</td>\n",
       "      <td></td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262944</th>\n",
       "      <td>T40000_S006</td>\n",
       "      <td>A Mobile Phone based Speech Therapist</td>\n",
       "      <td>Speech therapy sessions require a patient to t...</td>\n",
       "      <td>Pandey/Pande/Kopparapu</td>\n",
       "      <td>cs.CY/cs.HC</td>\n",
       "      <td>2016-01-11</td>\n",
       "      <td></td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262945</th>\n",
       "      <td>T40000_S007</td>\n",
       "      <td>A Mobile Phone based Speech Therapist</td>\n",
       "      <td>Additionally, there is a severe shortage of tr...</td>\n",
       "      <td>Pandey/Pande/Kopparapu</td>\n",
       "      <td>cs.CY/cs.HC</td>\n",
       "      <td>2016-01-11</td>\n",
       "      <td></td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262946</th>\n",
       "      <td>T40000_S008</td>\n",
       "      <td>A Mobile Phone based Speech Therapist</td>\n",
       "      <td>In this paper, we propose a low cost mobile sp...</td>\n",
       "      <td>Pandey/Pande/Kopparapu</td>\n",
       "      <td>cs.CY/cs.HC</td>\n",
       "      <td>2016-01-11</td>\n",
       "      <td></td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262947</th>\n",
       "      <td>T40000_S009</td>\n",
       "      <td>A Mobile Phone based Speech Therapist</td>\n",
       "      <td>The proposed system, which is being built, ena...</td>\n",
       "      <td>Pandey/Pande/Kopparapu</td>\n",
       "      <td>cs.CY/cs.HC</td>\n",
       "      <td>2016-01-11</td>\n",
       "      <td></td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262948 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Id                                              Title  \\\n",
       "0       T00001_S001  Cheating-Resilient Incentive Scheme for Mobile...   \n",
       "1       T00001_S002  Cheating-Resilient Incentive Scheme for Mobile...   \n",
       "2       T00001_S003  Cheating-Resilient Incentive Scheme for Mobile...   \n",
       "3       T00001_S004  Cheating-Resilient Incentive Scheme for Mobile...   \n",
       "4       T00001_S005  Cheating-Resilient Incentive Scheme for Mobile...   \n",
       "...             ...                                                ...   \n",
       "262943  T40000_S005              A Mobile Phone based Speech Therapist   \n",
       "262944  T40000_S006              A Mobile Phone based Speech Therapist   \n",
       "262945  T40000_S007              A Mobile Phone based Speech Therapist   \n",
       "262946  T40000_S008              A Mobile Phone based Speech Therapist   \n",
       "262947  T40000_S009              A Mobile Phone based Speech Therapist   \n",
       "\n",
       "                                                 Abstract  \\\n",
       "0       Mobile Crowdsensing is a promising paradigm fo...   \n",
       "1       As a fundamental property of Mobile Crowdsensi...   \n",
       "2       Therefore, a mechanism is required for the sys...   \n",
       "3       In this paper, we develop a novel Cheating-Res...   \n",
       "4       Via theoretical analysis, we demonstrate the c...   \n",
       "...                                                   ...   \n",
       "262943  Speech therapy is critical for continuous impr...   \n",
       "262944  Speech therapy sessions require a patient to t...   \n",
       "262945  Additionally, there is a severe shortage of tr...   \n",
       "262946  In this paper, we propose a low cost mobile sp...   \n",
       "262947  The proposed system, which is being built, ena...   \n",
       "\n",
       "                        Authors   Categories Created Date noTask1  Position  \n",
       "0       Zhao/Yang/Yu/Yao/Lin/Li  cs.NI/cs.CR   2017-01-08                 1  \n",
       "1       Zhao/Yang/Yu/Yao/Lin/Li  cs.NI/cs.CR   2017-01-08                 2  \n",
       "2       Zhao/Yang/Yu/Yao/Lin/Li  cs.NI/cs.CR   2017-01-08                 3  \n",
       "3       Zhao/Yang/Yu/Yao/Lin/Li  cs.NI/cs.CR   2017-01-08                 4  \n",
       "4       Zhao/Yang/Yu/Yao/Lin/Li  cs.NI/cs.CR   2017-01-08                 5  \n",
       "...                         ...          ...          ...     ...       ...  \n",
       "262943   Pandey/Pande/Kopparapu  cs.CY/cs.HC   2016-01-11                 5  \n",
       "262944   Pandey/Pande/Kopparapu  cs.CY/cs.HC   2016-01-11                 6  \n",
       "262945   Pandey/Pande/Kopparapu  cs.CY/cs.HC   2016-01-11                 7  \n",
       "262946   Pandey/Pande/Kopparapu  cs.CY/cs.HC   2016-01-11                 8  \n",
       "262947   Pandey/Pande/Kopparapu  cs.CY/cs.HC   2016-01-11                 9  \n",
       "\n",
       "[262948 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:27.856176Z",
     "start_time": "2019-12-30T09:45:27.843184Z"
    }
   },
   "outputs": [],
   "source": [
    "class PaperDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in [\"train\", \"test\",\"val\"]  # 一般訓練你會需要 dev set\n",
    "        self.mode = mode\n",
    "        # 大數據你會需要用 iterator=True\n",
    "        if mode == \"train\":\n",
    "            self.df = train_df\n",
    "        elif mode == \"test\":\n",
    "            self.df = test_df\n",
    "        elif mode == \"val\":\n",
    "            self.df = validation_df\n",
    "        self.len = len(self.df)\n",
    "        #self.label_map = {'agreed': 0, 'disagreed': 1, 'unrelated': 2}\n",
    "        self.tokenizer = tokenizer  # 我們將使用 BERTall_df = pd.read_pickle('order_abstract.pkl') tokenizer\n",
    "    \n",
    "    # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == \"test\":\n",
    "            abstract = self.df['Abstract'][idx]\n",
    "            label_tensor = None\n",
    "        else:\n",
    "            abstract = self.df['Abstract'][idx]\n",
    "            label = np.array([0, 0, 0, 0, 0, 0])\n",
    "            temp = self.df.iloc[idx, 8:14].values\n",
    "            # 將 label 文字也轉換成索引方便轉換成 tensor\n",
    "            #label_id = self.label_map[label]\n",
    "            for i,x in enumerate(temp):\n",
    "                label[i] = int(temp[i])\n",
    "            label_tensor = torch.from_numpy(label)\n",
    "            \n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_abstract = self.tokenizer.tokenize(abstract)\n",
    "        word_pieces += tokens_abstract + [\"[SEP]\"]\n",
    "        len_a = len(word_pieces)\n",
    "        \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
    "        segments_tensor = torch.tensor([0] * len_a, \n",
    "                                        dtype=torch.long)\n",
    "        #get position tensor\n",
    "        posi_tensor = torch.tensor([float(self.df.iloc[idx, 7])], dtype=torch.float)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor, posi_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "    \n",
    "    \n",
    "# 初始化一個專門讀取訓練樣本的 Dataset，使用中文 BERT 斷詞\n",
    "trainset = PaperDataset(\"train\", tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:28.262826Z",
     "start_time": "2019-12-30T09:45:28.261368Z"
    }
   },
   "outputs": [],
   "source": [
    "#for i in range(df.shape[0]):\n",
    "#    df.iloc[i,7:] = df.iloc[i,7:].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:29.081930Z",
     "start_time": "2019-12-30T09:45:29.051973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[原始文本]\n",
      "句子 1：This paper first describes an `obfuscating' compiler technology developed for encrypted computing, then examines if the trivial case without encryption produces much-sought indistinguishability obfuscation.\n",
      "分類  ：['1' '1' '0' '0' '0' '0']\n",
      "posi:1\n",
      "\n",
      "--------------------\n",
      "\n",
      "[Dataset 回傳的 tensors]\n",
      "tokens_tensor  ：tensor([  101,  1188,  2526,  1148,  4856,  1126,   169,   184,  1830, 14703,\n",
      "        26996,  1916,   112, 26012,  2815,  1872,  1111,  4035,  1665,  1616,\n",
      "        15514, 12783,   117,  1173, 22987,  1191,  1103, 23594,  1692,  1443,\n",
      "        26463,  6570,  1277,   118,  4110,  1107, 10396,  1916,  6592,  5480,\n",
      "         5474,   184,  1830, 14703, 26996,  2116,   119,   102])\n",
      "\n",
      "segments_tensor：tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "\n",
      "label_tensor   ：tensor([1, 1, 0, 0, 0, 0])\n",
      "\n",
      "posi: tensor([1.])\n",
      "\n",
      "--------------------\n",
      "\n",
      "[還原 tokens_tensors]\n",
      "[CLS] This paper first describes an ` o ##b ##fu ##sca ##ting ' compiler technology developed for en ##c ##ry ##pted computing , then examines if the trivial case without encryption produces much - sought in ##dis ##ting ##ui ##sha ##bility o ##b ##fu ##sca ##tion . [SEP]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 選擇第一個樣本\n",
    "sample_idx = 0\n",
    "\n",
    "# 將原始文本拿出做比較\n",
    "abstract = trainset.df['Abstract'][sample_idx]\n",
    "label = trainset.df.iloc[sample_idx,8:].values\n",
    "posi = trainset.df.iloc[sample_idx,7]\n",
    "\n",
    "# 利用剛剛建立的 Dataset 取出轉換後的 id tensors\n",
    "tokens_tensor, segments_tensor, label_tensor,posi_tensor = trainset[sample_idx]\n",
    "\n",
    "# 將 tokens_tensor 還原成文本\n",
    "tokens = tokenizer.convert_ids_to_tokens(tokens_tensor.tolist())\n",
    "combined_text = \" \".join(tokens)\n",
    "\n",
    "# 渲染前後差異，毫無反應就是個 print。可以直接看輸出結果\n",
    "print(f\"\"\"[原始文本]\n",
    "句子 1：{abstract}\n",
    "分類  ：{label}\n",
    "posi:{posi}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[Dataset 回傳的 tensors]\n",
    "tokens_tensor  ：{tokens_tensor}\n",
    "\n",
    "segments_tensor：{segments_tensor}\n",
    "\n",
    "label_tensor   ：{label_tensor}\n",
    "\n",
    "posi: {posi_tensor}\n",
    "\n",
    "--------------------\n",
    "\n",
    "[還原 tokens_tensors]\n",
    "{combined_text}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:29.459305Z",
     "start_time": "2019-12-30T09:45:29.449981Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 這個函式的輸入 `samples` 是一個 list，裡頭的每個 element 都是\n",
    "# 剛剛定義的 `FakeNewsDataset` 回傳的一個樣本，每個樣本都包含 3 tensors：\n",
    "# - tokens_tensor\n",
    "# - segments_tensor\n",
    "# - label_tensor\n",
    "# 它會對前兩個 tensors 作 zero padding，並產生前面說明過的 masks_tensors\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 測試集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = [s[2] for s in samples]\n",
    "    else:\n",
    "        label_ids = None\n",
    "    posi_tensors = [s[3] for s in samples]\n",
    "    \n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    if label_ids != None:\n",
    "        label_ids = pad_sequence(label_ids, \n",
    "                                        batch_first=True)\n",
    "    posi_tensors = pad_sequence(posi_tensors, \n",
    "                                    batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(\n",
    "        tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, posi_tensors, label_ids\n",
    "\n",
    "\n",
    "# 初始化一個每次回傳 64 個訓練樣本的 DataLoader\n",
    "# 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵\n",
    "BATCH_SIZE = 16\n",
    "trainloader = DataLoader(trainset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:29.931252Z",
     "start_time": "2019-12-30T09:45:29.904484Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([16, 48]) \n",
      "tensor([[  101,  1188,  2526,  1148,  4856,  1126,   169,   184,  1830, 14703,\n",
      "         26996,  1916,   112, 26012,  2815,  1872,  1111,  4035,  1665,  1616,\n",
      "         15514, 12783,   117,  1173, 22987,  1191,  1103, 23594,  1692,  1443,\n",
      "         26463,  6570,  1277,   118,  4110,  1107, 10396,  1916,  6592,  5480,\n",
      "          5474,   184,  1830, 14703, 26996,  2116,   119,   102],\n",
      "        [  101,  1706,  7098,  1142,  2463,   117,  1195, 17573,  1103,  2209,\n",
      "          1359,  4035, 13775,  1197,   118,  1260, 13775,  1197,  2235,  1115,\n",
      "         15294,  9988,  2838,  4351,  1121,  1160,   118,  8611,  9726,  1116,\n",
      "          1106,  1141,   118,  8611,  2001,  1942,  1162,  3190,  8409,   119,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1130,  1142,  2526,   117,  1195,  1675,  1126,  8362,  6385,\n",
      "          3365, 16641,  1181,  3776,  8297,  1111, 23389,  2619,  1105, 10393,\n",
      "          1107, 10900,  6581,   119,   102,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  6940,   117,  1195,  1294,  1142,  4220,  6736,  1111,  5093,\n",
      "           118,  4442,  4048,  1118, 17827,  1103, 16516, 24984, 13597,  1106,\n",
      "          1294,  1103,  4321,   118, 19795,  4453,  2235,  2964,   113,  1107,\n",
      "          2538,  1104,  1103,  1295,  1104, 11934,   114,   119,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1247,  4056,   170,  2043,  2783,  1104,  1472,  8015,  1111,\n",
      "         10005,  1158,  4683,  1259,  1103,  1119, 26900,  4884,   117,  5250,\n",
      "          2822, 15197,  5562,  3584,   117,  1105,  2361, 20844,  5970, 10340,\n",
      "          4571, 14975,   119,   102,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1109,  1449, 16621,  1116,  1213,   118,  1103,   118,  4705,\n",
      "         27553, 20629,  1988,  1105,  7593,  1105,  1256,  8468, 22308,  1174,\n",
      "          2191,   118,  7593,  1165,  1940,  1116,  1665, 17223, 25583,  3296,\n",
      "           119,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1337,  1110,   117,  1195,  2810,  1106,  1267,   170,  1353,\n",
      "          1849,  1107,  1103,  2672,  9479,  1114,  4161,  1106,   170, 19353,\n",
      "         24211,  1891,  1113,  1103,  7758,  4344,   119,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101, 28009,  1116,  1132,  1982,  1113,  2684,  1942,  2137,  5828,\n",
      "          6059,  1339,  2233,  9388,  1105, 12120,  1116, 13830,  3673, 25246,\n",
      "          1107,  1103,  5469,   113,   141,  2271,  2924,   114,  2233,  9388,\n",
      "           119,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1109,  3594,  5072,  1115, 10875,  1103,  9111,  1104,  2344,\n",
      "          3411,  1112,   170,  1692,  2025,  1106,  1437,  1293, 11108, 17483,\n",
      "          3752,  4625,  4275,  1120,  1103,  2150,  1104,  1103,  1902,  1965,\n",
      "          1169,  1129,  7042,  1219,  1103,  1718,  4065,   119,   102,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1284,  1145,  1675,  1103, 14482,  1114,  1103, 11112, 19548,\n",
      "         21254,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1130,  1901,   117,   141,   118,   187,  1475,  2137,  2162,\n",
      "          1169,  2194,  1842,   118,  1159,  4795, 13032,  1105,  2686,  5173,\n",
      "          2734,  1134,  1132,  9301,  1111,  1415,   118,  3418,  2233,  3622,\n",
      "           119,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1409,  1195,  3189,  1103,  4469, 12745,  1104,  1251,  1104,\n",
      "          1103,  3594,  3752,  2648,  1506,  1103,  1362,   117,  1297,   118,\n",
      "          1263,  3776,  1110,  1141,  1104,  1172,   119,   102,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1130,  1901,   117,  1103,  7626,  1104, 12893,  2233, 27948,\n",
      "          1110, 18372,   117,  1105,  1954,  1352,   118,  1104,   118,  1103,\n",
      "           118,  1893,  1996,  3776, 14975,  2834,  2235,  1155,  1103, 18501,\n",
      "          7866,   119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  3278, 10114, 11679,  1361, 10414,  2087,  6966,   117,  1142,\n",
      "          9219,  3114,  6803, 21852, 26463,   119,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1284, 24443,  6834,  1162,  1103,  2629,  1104,  1472,  2443,\n",
      "          4220,  1116,   117,  2235,  3211,  1105,  4321, 26996,  1513,  6165,\n",
      "           117,  1105,  1437,  1115,  1242,  9959,  1189,  1113,  1103,  4579,\n",
      "          1104,  5393,  1202,  1136,  1579,  4036,  1106,  1142,  1167,  2703,\n",
      "          4579,   119,   102,     0,     0,     0,     0,     0],\n",
      "        [  101,  1573,  1677,   117,  1292, 12182,  1105,  9652,  1956,  1144,\n",
      "          1136,  1151,  4572,  1107,   170, 13943,  8297,  1106, 11407,  1103,\n",
      "          2112,  2988, 17030,  8569,  2463,   119,   102,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([16, 48])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([16, 48])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 0, 0, 0,"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>limit_output extension: Maximum message size of 10000 exceeded with 12357 characters</b>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "threshold = 0.3\n",
    "tokens_tensors, segments_tensors, \\\n",
    "    masks_tensors, posi_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "posi_tensors.shape        = {posi_tensors.shape}\n",
    "{label_ids}\n",
    "------------------------\n",
    "label_ids.shape           = {label_ids.shape}\n",
    "{posi_tensors}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:30.366949Z",
     "start_time": "2019-12-30T09:45:30.357641Z"
    }
   },
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config, num_labels=6):\n",
    "        super(BertForSequenceClassification, self).__init__(config)\n",
    "        self.num_labels = num_labels\n",
    "        self.bert = BertModel(config)  # 載入預訓練 BERT\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # 簡單 linear 層++++++++++++++++++\n",
    "        self.classifier = nn.Linear(config.hidden_size+1, num_labels)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, position=None, labels=None):\n",
    "        # BERT 輸入就是 tokens, segments, masks\n",
    "        _,pooled_output = self.bert(input_ids, token_type_ids, attention_mask)\n",
    "#         print(position.shape)\n",
    "#         print(pooled_output.shape)\n",
    "        if position is not None:\n",
    "            pooled_output=torch.cat((position, pooled_output),1)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "#         print(pooled_output)\n",
    "        # 線性分類器將 dropout 後的 BERT repr. 轉成類別 logits\n",
    "        logits = self.classifier(pooled_output)\n",
    "        #logits = logits.sigmoid()\n",
    "        #print(logits)\n",
    "        \n",
    "        # 輸入有 labels 的話直接計算 Cross Entropy 回傳，方便！\n",
    "        if labels is not None:\n",
    "            labels = labels.float()\n",
    "            #print(labels)\n",
    "            #print(logits)   #i may need to do mask in bottom\n",
    "            ##for i in range(len(logits)):\n",
    "            ##    for j in range(6):\n",
    "            ##        if logits[i][j] >= threshold:\n",
    "            ##            logits[i][j] = 1\n",
    "            ##        else:\n",
    "            ##            logits[i][j] = 0\n",
    "            \n",
    "            loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "            #print(logits.view(-1, self.num_labels),\" \",labels.view(-1, self.num_labels))\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1, self.num_labels))\n",
    "            return loss\n",
    "        # 回傳各類別的 logits\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:33.850450Z",
     "start_time": "2019-12-30T09:45:30.832279Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=769, out_features=6, bias=True)\n"
     ]
    }
   ],
   "source": [
    "#classifier\n",
    "PRETRAINED_MODEL_NAME = \"bert-base-cased\"\n",
    "NUM_LABELS = 6\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRETRAINED_MODEL_NAME, num_labels=NUM_LABELS)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# high-level 顯示此模型裡的 modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:36.328755Z",
     "start_time": "2019-12-30T09:45:33.851529Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    counter = 0\n",
    "    score = 0\n",
    "    threshold = 0.3\n",
    "    model.eval()  # 推論模式\n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            tokens_tensors, segments_tensors, masks_tensors, posi_tensors, = data[:4]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors,\n",
    "                            position=posi_tensors)\n",
    "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            logits = outputs\n",
    "            #檢查如果沒有分類就把它分到值最高的那一類\n",
    "            for i in range(len(logits)):\n",
    "                    maxp = 0\n",
    "                    all0 = True\n",
    "                    for j in range(len(logits[i])):\n",
    "                        if(outputs[i][j] > outputs[i][maxp]): maxp = j\n",
    "                        if logits[i][j] >= threshold:\n",
    "                            logits[i][j] = 1\n",
    "                            all0 = False\n",
    "                        else:\n",
    "                            logits[i][j] = 0\n",
    "                    if(all0):\n",
    "                        logits[i][maxp] = 1\n",
    "            pred = logits\n",
    "#             print(pred)\n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[4]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                for i in range(len(pred)):\n",
    "                    score += metrics.f1_score(labels[i].cpu().numpy(), pred[i].cpu().numpy(), average=\"micro\")\n",
    "                #print(pred[0], \"---\", labels[0], \"----\", len(pred))\n",
    "            # 將當前 batch 記錄下來\n",
    "            predictions.append(pred)\n",
    "            counter += 1\n",
    "            #break\n",
    "    if compute_acc:\n",
    "        acc = score / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "    \n",
    "# 讓模型跑在 GPU 上並取得訓練集的分類準確率\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "#_, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "#print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:36.332131Z",
     "start_time": "2019-12-30T09:45:36.329931Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:36.340841Z",
     "start_time": "2019-12-30T09:45:36.332999Z"
    }
   },
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load(\"bert.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:59.036570Z",
     "start_time": "2019-12-30T09:45:36.341681Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "validation_set = PaperDataset(\"val\", tokenizer=tokenizer)\n",
    "validation_loader = DataLoader(validation_set, batch_size=16, \n",
    "                        collate_fn=create_mini_batch)\n",
    "_, vacc = get_predictions(model, validation_loader, compute_acc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T09:45:59.039761Z",
     "start_time": "2019-12-30T09:45:59.037668Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6940473650522533\n"
     ]
    }
   ],
   "source": [
    "print(vacc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T10:41:26.156642Z",
     "start_time": "2019-12-30T09:46:00.976828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] loss: 0.321, acc: 0.879, validation acc: 0.868\n",
      "[epoch 2] loss: 0.264, acc: 0.902, validation acc: 0.870\n",
      "[epoch 3] loss: 0.202, acc: 0.934, validation acc: 0.868\n",
      "[epoch 4] loss: 0.132, acc: 0.960, validation acc: 0.867\n",
      "CPU times: user 41min, sys: 11min 56s, total: 52min 56s\n",
      "Wall time: 55min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.train()\n",
    "\n",
    "# 使用 Adam Optim 更新整個分類模型的參數\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1.0e-5)\n",
    "\n",
    "\n",
    "EPOCHS = 4  # 幸運數字\n",
    "for epoch in range(EPOCHS):\n",
    "    output_dir = str(epoch + 1)\n",
    "    running_loss = 0.0\n",
    "    counter = 0.0\n",
    "    for data in trainloader:\n",
    "        \n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, posi_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels,\n",
    "                        position=posi_tensors\n",
    "                        )\n",
    "        loss = outputs\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "        # 紀錄當前 batch loss\n",
    "        # 可能可以改成算mean而不是sum\n",
    "        running_loss += loss.item()\n",
    "        counter += 1\n",
    "    # 計算分類準確率\n",
    "    _, acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "    #model_to_save = model.module if hasattr(model, 'module') else model\n",
    "    #model_to_save.save_pretrained(output_dir)\n",
    "    des = \"bert\" + str(epoch+1) + \".pt\"\n",
    "    torch.save(model.state_dict(), des)\n",
    "    #torch.save(,os.path.join(output_dir, 'training_args.bin'))\n",
    "    _, vacc = get_predictions(model, validation_loader, compute_acc=True)\n",
    "    print('[epoch %d] loss: %.3f, acc: %.3f, validation acc: %.3f' %\n",
    "          (epoch + 1, running_loss/counter , acc, vacc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T18:34:17.396099Z",
     "start_time": "2019-12-29T18:34:17.394373Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), \"bert.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T11:37:53.206950Z",
     "start_time": "2019-12-30T11:18:47.850669Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"bert2.pt\"))\n",
    "testset = PaperDataset(\"test\", tokenizer=tokenizer)\n",
    "testloader = DataLoader(testset, batch_size=32, \n",
    "                        collate_fn=create_mini_batch)\n",
    "# 用分類模型預測測試集\n",
    "predictions = get_predictions(model, testloader)\n",
    "res = predictions[0]\n",
    "for i in range(len(predictions)):\n",
    "    if i > 0:\n",
    "        res = torch.cat((res,predictions[i]),0)\n",
    "# 用來將預測的 label id 轉回 label 文字\n",
    "# 生成 Kaggle 繳交檔案\n",
    "df = pd.DataFrame({\"Category\": res.tolist()})\n",
    "df_pred = pd.concat([testset.df.loc[:, [\"Id\"]], \n",
    "                          df.loc[:, 'Category']], axis=1)\n",
    "#df_pred.to_csv('bert_1_prec_training_samples.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T11:37:53.651057Z",
     "start_time": "2019-12-30T11:37:53.208278Z"
    }
   },
   "outputs": [],
   "source": [
    "list__ = [[],[],[],[],[],[]]\n",
    "for d in df_pred['Category']:\n",
    "    for j in range(6):\n",
    "        list__[j].append(int(d[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T11:37:53.862352Z",
     "start_time": "2019-12-30T11:37:53.652645Z"
    }
   },
   "outputs": [],
   "source": [
    "df_pred['BACKGROUND'] = list__[0]\n",
    "df_pred['OBJECTIVES'] = list__[1]\n",
    "df_pred['METHODS'] = list__[2]\n",
    "df_pred['RESULTS'] = list__[3]\n",
    "df_pred['CONCLUSIONS'] = list__[4]\n",
    "df_pred['OTHERS'] = list__[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T11:37:53.865434Z",
     "start_time": "2019-12-30T11:37:53.863737Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T11:37:53.887749Z",
     "start_time": "2019-12-30T11:37:53.866455Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "      <th>BACKGROUND</th>\n",
       "      <th>OBJECTIVES</th>\n",
       "      <th>METHODS</th>\n",
       "      <th>RESULTS</th>\n",
       "      <th>CONCLUSIONS</th>\n",
       "      <th>OTHERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T00001_S001</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T00001_S002</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T00001_S003</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T00001_S004</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T00001_S005</td>\n",
       "      <td>[0.0, 0.0, 0.0, 1.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262943</th>\n",
       "      <td>T40000_S005</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262944</th>\n",
       "      <td>T40000_S006</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262945</th>\n",
       "      <td>T40000_S007</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262946</th>\n",
       "      <td>T40000_S008</td>\n",
       "      <td>[0.0, 1.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262947</th>\n",
       "      <td>T40000_S009</td>\n",
       "      <td>[1.0, 0.0, 0.0, 0.0, 0.0, 0.0]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262948 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Id                        Category  BACKGROUND  OBJECTIVES  \\\n",
       "0       T00001_S001  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]           1           0   \n",
       "1       T00001_S002  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]           1           0   \n",
       "2       T00001_S003  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]           0           1   \n",
       "3       T00001_S004  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]           0           1   \n",
       "4       T00001_S005  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0]           0           0   \n",
       "...             ...                             ...         ...         ...   \n",
       "262943  T40000_S005  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]           1           0   \n",
       "262944  T40000_S006  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]           1           0   \n",
       "262945  T40000_S007  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]           1           0   \n",
       "262946  T40000_S008  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0]           0           1   \n",
       "262947  T40000_S009  [1.0, 0.0, 0.0, 0.0, 0.0, 0.0]           1           0   \n",
       "\n",
       "        METHODS  RESULTS  CONCLUSIONS  OTHERS  \n",
       "0             0        0            0       0  \n",
       "1             0        0            0       0  \n",
       "2             0        0            0       0  \n",
       "3             0        0            0       0  \n",
       "4             0        1            0       0  \n",
       "...         ...      ...          ...     ...  \n",
       "262943        0        0            0       0  \n",
       "262944        0        0            0       0  \n",
       "262945        0        0            0       0  \n",
       "262946        0        0            0       0  \n",
       "262947        0        0            0       0  \n",
       "\n",
       "[262948 rows x 8 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T11:37:53.920370Z",
     "start_time": "2019-12-30T11:37:53.888853Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = df1.drop(columns='Category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T11:37:53.931848Z",
     "start_time": "2019-12-30T11:37:53.921554Z"
    }
   },
   "outputs": [],
   "source": [
    "df1= df1.rename(columns={\"Id\": \"order_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T11:37:53.941554Z",
     "start_time": "2019-12-30T11:37:53.933495Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>BACKGROUND</th>\n",
       "      <th>OBJECTIVES</th>\n",
       "      <th>METHODS</th>\n",
       "      <th>RESULTS</th>\n",
       "      <th>CONCLUSIONS</th>\n",
       "      <th>OTHERS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T00001_S001</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T00001_S002</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>T00001_S003</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>T00001_S004</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>T00001_S005</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262943</th>\n",
       "      <td>T40000_S005</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262944</th>\n",
       "      <td>T40000_S006</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262945</th>\n",
       "      <td>T40000_S007</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262946</th>\n",
       "      <td>T40000_S008</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262947</th>\n",
       "      <td>T40000_S009</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>262948 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           order_id  BACKGROUND  OBJECTIVES  METHODS  RESULTS  CONCLUSIONS  \\\n",
       "0       T00001_S001           1           0        0        0            0   \n",
       "1       T00001_S002           1           0        0        0            0   \n",
       "2       T00001_S003           0           1        0        0            0   \n",
       "3       T00001_S004           0           1        0        0            0   \n",
       "4       T00001_S005           0           0        0        1            0   \n",
       "...             ...         ...         ...      ...      ...          ...   \n",
       "262943  T40000_S005           1           0        0        0            0   \n",
       "262944  T40000_S006           1           0        0        0            0   \n",
       "262945  T40000_S007           1           0        0        0            0   \n",
       "262946  T40000_S008           0           1        0        0            0   \n",
       "262947  T40000_S009           1           0        0        0            0   \n",
       "\n",
       "        OTHERS  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "...        ...  \n",
       "262943       0  \n",
       "262944       0  \n",
       "262945       0  \n",
       "262946       0  \n",
       "262947       0  \n",
       "\n",
       "[262948 rows x 7 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T05:54:58.796514Z",
     "start_time": "2019-12-30T05:54:58.708416Z"
    }
   },
   "outputs": [],
   "source": [
    "# df2 = pd.read_csv(\"task1_sample_submission.csv\")\n",
    "# df2 = df2.loc[131166:]\n",
    "# df1 = pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-30T11:37:57.610356Z",
     "start_time": "2019-12-30T11:37:55.773014Z"
    }
   },
   "outputs": [],
   "source": [
    "df1.to_csv('result_bert_private_3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T18:44:03.473010Z",
     "start_time": "2019-12-29T18:43:36.206158Z"
    }
   },
   "outputs": [],
   "source": [
    "count__ = 0\n",
    "for i in range(131166):\n",
    "    if (df1.iloc[i,1:].values == [0,0,0,0,0,0]).all():\n",
    "        count__ += 1\n",
    "print(count__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T11:44:18.315043Z",
     "start_time": "2019-12-29T11:43:51.315518Z"
    }
   },
   "outputs": [],
   "source": [
    "list_ = []\n",
    "for idx in range(testset.df.shape[0]): \n",
    "    label = np.array([0, 0, 0, 0, 0, 0])\n",
    "    temp = testset.df.iloc[idx, 7:].values\n",
    "    for i,x in enumerate(temp):\n",
    "        label[i] = int(temp[i])\n",
    "    label_tensor = torch.from_numpy(label).float()\n",
    "    list_.append(label_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-29T11:44:25.625022Z",
     "start_time": "2019-12-29T11:44:23.054015Z"
    }
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i in range(df.shape[0]):\n",
    "    if(torch.equal(list_[i], torch.FloatTensor(df_pred['Category'][i]))):\n",
    "        counter += 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred[100:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
